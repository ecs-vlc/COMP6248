
\documentclass[a4paper]{article}
\usepackage[a4paper,top=2cm,bottom=2cm,left=2cm,right=2cm,marginparwidth=2cm]{geometry}
\usepackage{lmodern}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{textpos} % package for the positioning
\usepackage{tcolorbox}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.3em}

\usepackage{textcomp}
\begin{document}

\lstset{language=Python,upquote=true}

\setlength{\leftskip}{20pt}
\title{Lab 2 Exercise - PyTorch Autograd}
\author{Jonathon Hare (jsh2@ecs.soton.ac.uk)}

\maketitle

% \begin{abstract}
% \end{abstract}
% \tableofcontents

This is the exercise that you need to work through \textbf{on your own} after completing the second lab session. You'll need to write up your results/answers/findings and submit this to ECS handin as a PDF document along with the other lab exercises near the end of the module (1 pdf document per lab). 

We expect that you should aim to use one side of A4 to cover your responses to \emph{this} exercise. This exercise is worth 5\% of your overall module grade.

\section{Implement matrix factorisation using gradient descent (take 2)}\label{sgd}

Last week we implemented a low-rank matrix factorisation by considering gradient updates to a single element at a time. Let's revisit that problem, but this time we'll use PyTorch's AD framework to compute the gradients for us on the entire problem, and then use these with gradient descent algorithm. As a recap, the optimisation problem we are solving is:

\begin{equation}
	\min\limits_{\hat{\bm U}, \hat{\bm V}}( \| \bm A - \hat{\bm U}\hat{\bm V}^\top \|_{\rm F}^2 )
\end{equation}
where $\bm A \in \mathbb{R}^{m \times n}$, $\hat{\bm U} \in \mathbb{R}^{m \times r}$, $\hat{\bm V} \in \mathbb{R}^{n \times r}$ and $r<\min(m,n)$. 

\begin{tcolorbox}[title=1.1 Implement gradient-based factorisation using PyTorch's AD (1 mark)]
Implement the optimisation problem using PyTorch's automatic differentiation framework to compute the gradients associated with the tensors $\hat{\bm U}$ and $\hat{\bm V}$, by completing the following method:

\begin{lstlisting}[language=Python]
from typing import Tuple

def sgd_factorise_ad(A: torch.Tensor, rank: int, num_epochs=1000, 
	lr=0.01) -> Tuple[torch.Tensor, torch.Tensor]:
\end{lstlisting}

Remember, unlike last week you won't be updating an element at a time, but rather you'll compute and apply all the gradients of $\hat{\bm U}$ to $\hat{\bm U}$, and of $\hat{\bm V}$ to $\hat{\bm V}$ once per epoch. $\hat{\bm U}$ and $\hat{\bm V}$ should be initially created with uniform random values. To compute the squared frobenius norm loss (reconstruction loss), use \texttt{torch.nn.functional.mse\_loss} with \texttt{reduction='sum'}.
\end{tcolorbox}

\begin{tcolorbox}[title=1.2 Factorise and compute reconstruction error on real data (1 mark)]
Use the following code to download a dataset of 150 instances and 4 features into a mean-centered tensor called \texttt{data}:

\begin{lstlisting}[language=Python]
import pandas as pd
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases'
	+'/iris/iris.data', header=None)
data = torch.tensor(df.iloc[:, [0,1,2,3]].values)
data = data - data.mean(dim=0)
\end{lstlisting}

Use your \texttt{sgd\_factorise\_ad} function to compute the rank-2 factorisation of the \texttt{data} matrix using the default values for learning rate and number of epochs. What is the reconstruction loss? How does this reconstruction loss compare to the loss of a rank-2 reconstruction computed using a truncated Singular Value Decomposition of the same data?
\end{tcolorbox}

\begin{tcolorbox}[title=1.3 Compare against PCA (1 mark)]
Given that our \texttt{data} matrix consisted of mean-centered data points encoded in each row, then the $U$ matrix from SVD represents a projection of the data onto its principle directions (i.e. it's the result of applying PCA to data).

Create a scatter plot of the data projected onto the first two principle axes computed by SVD. Now create a second scatter plot from the data in matrix $\hat{\bm U}$. What do you observe? Can you infer a relationship between an orthogonal linear transform to a lower dimensional space which maximises variance (which is what PCA does), to minimising reconstruction error?
\end{tcolorbox}

\section{A simple MLP}\label{mlp}
We'll now turn our attention to a simple MLP implementation that we'll train with gradient descent. Normally, we would utilise other aspects of the PyTorch library to simplify this, however for this exercise we're just going to use raw tensors and the AD framework to track their gradients.

Firstly, we'll set up some training and validation data:
\begin{lstlisting}[language=Python]
	import pandas as pd
	df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases'
		+'/iris/iris.data', header=None)
	df = df.sample(frac=1) #shuffle

	# add label indices column
	mapping = {k: v for v, k in enumerate(df[4].unique())}  
	df[5] = df[4].map(mapping)

	# normalise data
	alldata = torch.tensor(df.iloc[:, [0,1,2,3]].values, dtype=torch.float)
	alldata = (alldata - alldata.mean(dim=0)) / alldata.var(dim=0)

	# create datasets
	targets_tr = torch.tensor(df.iloc[:100, 5].values, dtype=torch.long)
	targets_va = torch.tensor(df.iloc[100:, 5].values, dtype=torch.long)
	data_tr = alldata[:100]
	data_va = alldata[100:]
\end{lstlisting}

Generally speaking\footnote{except when dealing with recurrent models}, PyTorch expects the first axis of a tensor to be the batch axis; this means that in the data tensors above, each instance is in a row. The simple MLP you're going to build will have the following form:

\begin{lstlisting}[language=Python]
	logits = torch.relu(data @ W1 + b1) @ W2 + b2
\end{lstlisting}

where $\mathrm{W1} \in \mathbb{R}^{4,12}$, $\mathrm{W2} \in \mathbb{R}^{12,3}$, and both $\mathrm{b1}$ and $\mathrm{b2} \in \mathbb{R}$. 

\begin{tcolorbox}[title=2.1 Implement the MLP (1 mark)]
Given the above form of the MLP, write the code to perform 100 epochs of gradient descent with a learning rate of 0.01 on the training (\_tr) data. Use PyTorch's cross-entropy loss (\texttt{torch.nn.functional.cross\_entropy(logits, targets\_tr)}) as the objective. Initialise \texttt{W1} and \texttt{W2} with normally distributed random numbers, and \texttt{b1} and \texttt{b2} as zeros. You'll need to manually apply the gradient updates to \texttt{W1}, \texttt{W2}, \texttt{b1} and \texttt{b2} each epoch.
\end{tcolorbox}

\begin{tcolorbox}[title=2.2 Test the MLP (1 mark)]
Run your MLP on the training data, and compute both the training and validation accuracies at the end of training. Repeat this a few times and report what you observe. 
\end{tcolorbox}

\end{document}



