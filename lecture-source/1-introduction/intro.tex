\ifdefined\beamerclass
\else
    \def\beamerclass{beamer}
\fi
\documentclass[\beamerclass]{beamer}

\usepackage{pgfpages}
\mode<handout>{
	% \setbeamercolor{background canvas}{bg=black!20}
	\pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]
}

\usepackage{lmodern}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{textpos} % package for the positioning

\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\usetheme{Copenhagen}
\hypersetup{pdfstartview={Fit}}
\lstset{basicstyle=\small\ttfamily,breaklines=true}

\title[COMP6248 Deep Learning]{COMP6248 Differentiable Programming}
\subtitle{(and some Deep Learning)}
\author{Jonathon Hare, Kate Farrahi \& Xiaohao Cai}
\institute[]
{
  Vision, Learning and Control\\
  University of Southampton 
}
\date{}
\subject{Computer Science}
\useoutertheme{infolines}
\setbeamertemplate{headline}{} %remove headline
\setbeamertemplate{navigation symbols}{} %remove navigation symbols

\definecolor{darkblue}{RGB}{37,55,97}
\definecolor{mellowyellow}{RGB}{247,206,70}

\addtobeamertemplate{footnote}{\hskip -2em}{}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\begin{document}

\begin{frame}[plain]
        \begin{tikzpicture}[overlay, remember picture, shift={(current page.south west)},font={\fontfamily{Montserrat-TOsF}\selectfont}]
        \fill [mellowyellow,text=darkblue] (0,0) rectangle (\paperwidth, \paperheight);
        \draw (4,7) node [align=left,text=darkblue] {\Huge \begin{tabular}{l} \textbf{Differentiate} \\ \textbf{your} \\ \textbf{Objective} \end{tabular}};
        \draw (11,1) node [align=left,text=darkblue] {\includegraphics[scale=0.15]{../vlc.png}};
        \end{tikzpicture}
\end{frame}

\frame{
  \titlepage
}

\begin{frame}{pause}
\frametitle{Machine Learning - A Recap}
{\tiny All credit for this slide goes to Niranjan}\\
\vspace{5mm}
\begin{tabular}{ll}
Data & $\{\bm{x}_n, \bm{y}_n\}^N_{n=1} \qquad \{\bm{x}_n\}^N_{n=1}$ 
\vspace{3mm} \\ \pause
Function Approximator & $\bm{y} = f (\bm{x}, \bm{\theta}) + \nu$ 
\vspace{3mm} \\ \pause
Parameter Estimation & $E_0 = \sum^N_{n=1} \{\|\bm{y}_n - f (\bm{x}_n; \bm{\theta})\|\}^2$
\vspace{3mm} \\ \pause
Prediction & $\bm{\hat y}_{N+1} = f(\bm{x}_{N+1}, \bm{\hat \theta})$
\vspace{3mm} \\ \pause
Regularisation & $E_1 = \sum^N_{n=1} \{\|\bm{y}_n - f (\bm{x}_n; \bm{\theta})\|\}^2 + r(\|\bm\theta\|)$
\vspace{3mm} \\ \pause
Modelling Uncertainty & $p(\bm\theta|\{\bm x_n, \bm y_n\}_{n=1}^N)$
\vspace{3mm} \\ \pause
Probabilistic Inference & $\mathop{\mathbb{E}}[g(\bm\theta)] = \int g(\bm\theta)p(\bm\theta)d\bm\theta = \frac{1}{N_s}\sum_{n=1}^{N_s}g(\bm\theta^{(n)})$
\vspace{3mm} \\ \pause
Sequence Modelling & $\bm x_n = f(\bm x_{n-1}, \bm\theta)$
\end{tabular}
\vspace{5mm}
\end{frame}

\begin{frame}
\frametitle{What is Deep Learning?}

Deep learning is primarily characterised by function compositions: \\ \vspace{10mm}
\begin{itemize}
	\item<2-> Feedforward networks: $\bm{y} = f (g(\bm{x}, \bm\theta_g), \bm{\theta_f})$
	\begin{itemize}
		\item Often with relatively simple functions (e.g. $f(\bm x, \bm{\theta}_f) = \sigma(\bm{x}^\top \bm{\theta}_f)$)
	\end{itemize} \vspace{3mm}
	\item<3-> Recurrent networks: $\bm y_t = f(\bm y_{t-1}, \bm x_t, \bm\theta) = f(f(\bm y_{t-2}, \bm x_{t-1}, \bm\theta), \bm x_t, \bm\theta) = \dots$
\end{itemize}
\vspace{10mm}

\uncover<4->{
In the early days the focus of deep learning was on learning functions for classification. Nowadays the functions are much more general in their inputs and outputs.
}

\end{frame}

\begin{frame}
\frametitle{What is Differentiable Programming?}
	
\begin{itemize}
	\item<+-> Differentiable programming is a term coined by Yann Lecun\footnote{https://www.facebook.com/yann.lecun/posts/10155003011462143} to describe a superset of Deep Learning.
	\item<+-> Captures the idea that computer programs can be constructed of parameterised functional blocks in which the parameters are learned using some form of gradient-based optimisation.
	\begin{itemize}
		\item<+-> The implication is that we need to be able to compute gradients with respect to the parameters of these functional blocks. We'll start explore this in detail next week...
		\item<+-> The idea of Differentiable Programming also opens up interesting possibilities: 
		\begin{itemize}
			\item The functional blocks don't need to be direct functions in a mathematical sense; more generally they can be \emph{algorithms}.
			\item What if the functional block we're learning parameters for is itself an algorithm that optimises the parameters of an internal algorithm using a gradient based optimiser?!\footnote{See our ICLR 2019 paper: https://arxiv.org/abs/1812.03928 and NeurIPS 2019 paper: https://arxiv.org/abs/1906.06565}
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Is all Deep Learning Differentiable Programming?}
\begin{itemize}
	\item Not necessarily!
	\begin{itemize}
		\item<+-> Most deep learning systems are trained using first order gradient-based optimisers, but there is an active body of research on gradient-free methods.
		\item<+-> There is an increasing interest in methods that use different styles of learning, such as Hebbian learning, within deep networks. More broadly there are a number of us\footnote{including at least myself, my PhD students and Geoff Hinton!} who are interested in biologically motivated models and learning methods.
		\begin{itemize}
			\item<+-> There's a lot of recent research that computes biological proxies for gradients though!
		\end{itemize}

		\item<+-> This course will primarily focus on differentiable methods, but we'll look at how relaxations can be made to make non-differentiable operators learnable with gradient-based optimisers.
	\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Why should we care about this?}
	\centering \includegraphics[width=0.9\textwidth]{Fig1.pdf}\blfootnote{Reference: Andrew Ng}
\end{frame}

\begin{frame}
	%\frametitle{Where did it all start \& what was the motivation?}
	\frametitle{Success stories - Object detection and segmentation}
	\centering \includegraphics[width=0.3\textwidth]{objseg.pdf}\blfootnote{Pinheiro, Pedro O., et al. "Learning to refine object segments." European Conference on Computer Vision. Springer, 2016.}
\end{frame}

\begin{frame}
	\frametitle{Success stories - Image generation}
	\centering \includegraphics[width=0.8\textwidth]{imggen.pdf}\blfootnote{Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep convolutional generative adversarial networks." arXiv preprint arXiv:1511.06434 (2015).}
\end{frame}

\begin{frame}
	%\frametitle{Where did it all start \& what was the motivation?}
	\frametitle{Success stories - Translation}

ENGLISH TEXT\\
The reason Boeing are doing this is to cram more seats in to make their plane
more competitive with our products," said Kevin Keniston, head of passenger
comfort at Europe's Airbus.
\\[1em]
TRANSLATED TO FRENCH\\
La raison pour laquelle Boeing fait cela est de creer plus de sieges pour rendre
son avion plus competitif avec nos produits", a declare Kevin Keniston, chef
du confort des passagers chez Airbus. \blfootnote{Wu, Yonghui, et al. "Google's neural machine translation system: Bridging the gap between human and machine translation." arXiv preprint arXiv:1609.08144 (2016).}

\end{frame}

\begin{frame}
	\frametitle{What is the objective of this module?}
	
	\textbf{A word of warning: This is not a module about how to apply someone else's deep network architecture to a task, or how to train existing models!}
	\\[1em]
	You will learn some of that along the way of course, but the real objective is for you to graduate knowing how to understand, critique and implement new and recent research papers on deep learning and associated topics.
\end{frame}

\begin{frame}
	\frametitle{What is the objective of this module?}
	\begin{itemize}
		\item<+-> To gain an in-depth theoretical and practical understanding of modern deep neural networks and their applications.
		\item<+-> Understand the underlying mathematical and algorithmic principles of deep learning.
		\item<+-> Understand the key factors that have made differentiable programming successful for various applications.
		\item<+-> Apply existing deep learning models to real datasets.
		\item<+-> Gain facility in working with deep learning libraries in order to create and evaluate network architectures.
		\item<+-> Critically appraise the merits and shortcomings of model architectures on specific problems.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{How is this module going to be delivered?}
	
	\begin{itemize}
		\item<+-> Lectures (2 per week)
		\begin{itemize}
			\item Note: I am refreshing some material from last year, but the website still has links to the old slides.
			\item You need to read the suggested papers/links before the lectures!
			\item There is a little room for some flexibility later in the course on topics - tell me what you're interested in!
			\item<+-> Lectures will be livestreamed via Teams, but also recorded for the website. 
			\item<+-> We need you to help: 
			\begin{itemize}
				\item<+-> \textbf{Reminders about hitting both record buttons at the beginning!}
				\item<+-> Ask questions via the chat --- Kate will be monitoring and interrupting me as needed to give a much richer lecture experience
				\item<+-> Tell us whats working and not working
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{How is this module going to be delivered?}

	\begin{itemize}
		\item<+-> Labs (1x 2 hour session per week for 8 weeks + additional help sessions)
		\begin{itemize}
			\item Labs consist of a number of Juypter notebooks you will work though.
			\begin{itemize}
				\item You'll be using PyTorch as the primary framework, with Torchbearer to help out.
				\item You will need to utilise GPU-compute for the later labs (we provide Google Colab links so you can use NVidia K80s in the cloud, but you'll also be able to use the RTX2070 in the lab machines if you wish).
			\end{itemize}
			\item I have scheduled lab sessions so you can get help and ask questions from myself and the demonstrator team.
			\item After each lab you will have to do a follow-up exercise that will be marked.
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{What will we cover in the module?}
	\url{http://comp6248.ecs.soton.ac.uk/}
\end{frame}

\begin{frame}
	\frametitle{Lab session plan}
	
	\begin{center}
	\begin{tabular}{ l l l }
		 Lab & Date & Topic \\ \hline
		 Lab 1  & 11/02/21 & Introducing PyTorch \\ 
		 Lab 2  & 19/02/21 & Automatic Differentiation \\  
		 Lab 3  & 25/02/21 & Optimisation \\
		 Lab 4  & 04/03/21 & NNs with PyTorch and Torchbearer \\
 		 Lab 5  & 11/03/21 & CNNs with PyTorch and Torchbearer \\
 		 Lab 6  & 18/03/21 & Transfer Learning \\
 		 \hline
 		 & Break & \\
		 \hline
		 Lab 7  & 15/04/21 & RNNs, Sequence Prediction and Embeddings \\
		 Lab 8  & 22/04/21 & Deep Generative Models \\
		        & 29/04/21 & Coursework Help and Advice \\
 		 	    & 06/05/21 & Coursework Help and Advice \\
	\end{tabular}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{What do we expect you already know?}
	
	\begin{itemize}
	\item<+-> COMP3223 or COMP6245 (fundamentals of statistical learning, MLPs, gradient descent, how to train and evaluate learning machines, supervised-vs-unsupervised)
	\item<+-> Fundamentals of:
	\begin{itemize}
		\item Matrix Algebra (matrix-matrix, matrix-vector and matrix-scalar operations, inverse, determinant, rank, Eigendecomposition, SVD);
		\item Probability \& Statistics (1st-order summary statistics, simple continous and discrete probability distributions, expected values, etc); and, 
		\item Multivariable Calculus (partial differentiation, chain-rule).
	\end{itemize}
	\item<+-> Programming in Python
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{What might you already know?}	
	\begin{itemize}
	\item<+-> How to use a deep learning framework (Keras, Tensorflow, PyTorch)?
	\item<+-> How to train an existing model architecture using a GPU?
	\item<+-> How to perform transfer learning?
	\item<+-> How to perform differentiable sampling of a Multivariate Normal Distribution?
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Assessment Structure}
	\begin{itemize}
		\item Lab work $40\%$ - Handin in week 10 (29th April, 4PM)
		\item Final project $40\%$  - Handin in Week 12 (13th May, 4PM) (+ interim handin in week 5)
		\item Online quizzes $20\%$ - Planned for Wednesdays in Weeks 6 (10th-Mar) and 11 (05-May) 
	\end{itemize}
\end{frame}

% \begin{frame}
% 	\frametitle{Assessment Timetable}
	
% 	\begin{center}
% 	\begin{tabular}{ l c  c}
% 		 Assessment & Date  & Time\\ \hline
% 		 Labs 1-3  &  22/02/19 & 16:00 \\ 
% 		 Coursework Team Information & 27/02/19 & 16:00 \\
% 		 In - class Test 1 & 01/03/19 & midnight  \\		 
% 		 Labs 4-6  & 15/03/19 & 16:00 \\  
% 		 In - class Test 2 & 29/03/19 & midnight  \\		 
% 		 Labs 7-8 & 03/05/19 & 16:00   \\
% 		 Final Coursework Submission & 15/05/19 & 16:00 \\
% 	\end{tabular}
% 	\end{center}
	
% \end{frame}

\begin{frame}
	\frametitle{The Main Assignment}
	\framesubtitle{The ICLR Reproducibility Challenge}
	\url{http://comp6248.ecs.soton.ac.uk/coursework.html}
\end{frame}

\end{document}