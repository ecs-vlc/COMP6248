\ifdefined\beamerclass
\else
    \def\beamerclass{beamer}
\fi
\documentclass[\beamerclass]{beamer}

\usepackage{pgfpages}
\mode<handout>{
  % \setbeamercolor{background canvas}{bg=black!20}
  \pgfpagesuselayout{2 on 1}[a4paper,border shrink=5mm]
}

\usepackage{lmodern}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{textpos} % package for the positioning

\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}

\usetheme{Copenhagen}
\hypersetup{pdfstartview={Fit}}
\lstset{basicstyle=\small\ttfamily,breaklines=true}

\usepackage[english]{babel}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
%\graphicspath{{./images/}}
\usepackage{tikz, pgf}
\usepackage{pgfplots}
\usetikzlibrary{shapes.geometric, arrows,chains}
\usepackage{booktabs,makecell,multirow,tabularx}
\usepackage{verbatim}
\renewcommand{\arraystretch}{1.2}
\renewcommand\theadfont{\normalfont\bfseries}
\usepackage{array}
\usepackage{listings}
\lstset{language=Java, showstringspaces=false}
\usepackage[normalem]{ulem}
\usepackage{bm}
\def\layersep{2.5cm}

\usepackage{xcolor}
%\usepackage{subfig}
\setbeamertemplate{caption}{\insertcaption}
\usepackage[caption=false]{subfig}
\usepackage{hyperref}
\usepackage{verbatim}
%\setbeamertemplate{caption}[numbered]%\numberwithin{figure}{section}
% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=3em, text centered, rounded corners, minimum height=3em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse, fill=red!20, node distance=3cm,
    minimum height=2em]
\tikzset{
  startstop/.style={
    rectangle, 
    rounded corners,
    minimum width=3cm, 
    minimum height=1cm,
    align=center, 
    draw=black, 
    fill=red!30
    },
  process/.style={
    rectangle, 
    minimum width=3cm, 
    minimum height=1cm, 
    align=center, 
    draw=black, 
    fill=blue!30
    },
  decision/.style={
    rectangle, 
    minimum width=3cm, 
    minimum height=1cm, align=center, 
    draw=black, 
    fill=green!30
    },
  arrow/.style={thick,->,>=stealth},
  dec/.style={
    ellipse, 
    align=center, 
    draw=black, 
    fill=green!30
    },
}
\tikzstyle{arrow} = [thick,->,>=stealth]

\tikzset{onslide/.code args={<#1>#2}{%
  \only<#1>{\pgfkeysalso{#2}} % \pgfkeysalso doesn't change the path
}}

\makeatletter
\newenvironment<>{btHighlight}[1][]
{\begin{onlyenv}#2\begingroup\tikzset{bt@Highlight@par/.style={#1}}\begin{lrbox}{\@tempboxa}}
{\end{lrbox}\bt@HL@box[bt@Highlight@par]{\@tempboxa}\endgroup\end{onlyenv}}

\newcommand<>\btHL[1][]{%
  \only#2{\begin{btHighlight}[#1]\bgroup\aftergroup\bt@HL@endenv}%
}
\def\bt@HL@endenv{%
  \end{btHighlight}%   
  \egroup
}
\newcommand{\bt@HL@box}[2][]{%
  \tikz[#1]{%
    \pgfpathrectangle{\pgfpoint{1pt}{0pt}}{\pgfpoint{\wd #2}{\ht #2}}%
    \pgfusepath{use as bounding box}%
    \node[anchor=base west, fill=orange!30,outer sep=0pt,inner xsep=1pt, inner ysep=0pt, rounded corners=3pt, minimum height=\ht\strutbox+1pt,#1]{\raisebox{1pt}{\strut}\strut\usebox{#2}};
  }%
}
\makeatother

\definecolor{darkblue}{RGB}{37,55,97}
\definecolor{mellowyellow}{RGB}{247,206,70}
\definecolor{almostwhite}{RGB}{254,255,255}
\definecolor{merrygreen}{RGB}{79,173,91}
\definecolor{funkyorange}{RGB}{240,154,56}

\addtobeamertemplate{footnote}{\hskip -2em}{}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\ReLU}{ReLU}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Formatting for title page
\title[Optimisation]{Optimisation}
\author{Kate Farrahi (adapted from Jon Hare's lecture)}  
\institute[]
{
  Vision, Learning and Control\\
  University of Southampton 
}
\date{}
\subject{Computer Science}
\useoutertheme{infolines}
\setbeamertemplate{headline}{} %remove headline
\setbeamertemplate{navigation symbols}{} %remove navigation symbols

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{frame}[plain]
  \begin{tikzpicture}[overlay, remember picture, shift={(current page.south west)},font={\fontfamily{Montserrat-TOsF}\selectfont}]
  \fill [mellowyellow,text=darkblue] (0,0) rectangle (\paperwidth, \paperheight);
  \draw (4,7) node [align=left,text=darkblue] {\Huge \begin{tabular}{l} \textbf{Minimise} \\ \textbf{your} \\ \textbf{Loss} \end{tabular}};
  \draw (11,1) node [align=left,text=darkblue] {\includegraphics[scale=0.15]{../vlc.png}};
  \end{tikzpicture}
\end{frame}


\begin{frame}
  \titlepage%\footnote{Some of the material in this lecture is based on Andrew Ng's lectures on Optimisation} %% \url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf }} 
\end{frame}
%%-------------------------------------------------------------%

%\begin{frame}
%  \frametitle{Gradient descent and SGD (again), and mini-batch SGD}
%  
%  We'll start up by looking again at gradient descent algorithms and their behaviours...
%\end{frame}

%-------------------------------------------------------------%
\begin{frame}[fragile]\frametitle{Reminder: Gradient Descent}

\begin{itemize}
  \item Define total loss as $\mathcal{L} = \sum_{(\bm x,y) \in \bm D} \ell(g(\bm x,\bm\theta), y)$ for some loss function $\ell$, dataset $\bm D$ and model $g$ with learnable parameters $\bm\theta$.
  \item Define how many passes over the data to make (each one known as an Epoch)
  \item Define a learning rate $\eta$
\end{itemize}

Gradient Descent updates the parameters $\bm\theta$ by moving them in the direction of the negative gradient with respect to the \textbf{total loss} $\mathcal{L}$ by the learning rate $\eta$ multiplied by the gradient:
\\[1em]
\hspace{1cm} \texttt{for each Epoch:}\\
\hspace{2cm} $\bm\theta \leftarrow \bm\theta - \eta \nabla_{\bm\theta} \mathcal{L}$
\end{frame}

%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{Gradient Descent}
\begin{itemize}
	\item<1-> Fewer updates to the model means more computationally efficient.
	\item<1-> The decreased update frequency results in more stable error gradient and more stable convergence.
 	\item<1-> The updates at the end of the training epoch require the additional complexity of accumulating prediction errors across all training examples.
	Commonly, batch gradient descent is implemented in such a way that it requires the entire training dataset in memory and available to the algorithm.
	\item<1-> Model updates, and in turn training speed, may become very slow for large datasets.
\end{itemize}
\end{frame}

%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{Reminder: Stochastic Gradient Descent}

\begin{itemize}
  \item Define loss function $\ell$, dataset $\bm D$ and model $g$ with learnable parameters $\bm\theta$.
  \item Define how many passes over the data to make (each one known as an Epoch)
  \item Define a learning rate $\eta$
\end{itemize}

Stochastic Gradient Descent updates the parameters $\bm\theta$ by moving them in the direction of the negative gradient with respect to the loss of a \textbf{single item} $\ell$ by the learning rate $\eta$ multiplied by the gradient:
\\[1em]
\hspace{1cm} \texttt{for each Epoch:}\\
  \hspace{2cm} \texttt{for each $(\bm x,y) \in \bm D$:}\\
    \hspace{3cm} $\bm\theta \leftarrow \bm\theta - \eta \nabla_{\bm\theta} \ell$
\end{frame}

%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{Stochastic Gradient Descent}
\begin{itemize}
	\item<1-> The frequent updates immediately give an insight into the performance of the model.
	\item<1-> The noisy update process can allow the model to avoid local minima.
  	\item<1-> The frequent updates can result in a noisy gradient signal, which may cause the model parameters and in turn the model error to jump around. This makes it hard for the algorithm to settle on an error minimum.
  	\item<1-> Computationally inefficient (poor utilisation of resources - particularly with respect to vectorisation)
\end{itemize}
\end{frame}

%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{Mini-batch Stochastic Gradient Descent}

\begin{itemize}
  \item Define a batch size $b$
  \item Define batch loss as $\mathcal{L}_b = \sum_{(\bm x,y) \in \bm D_b} \ell(g(\bm x,\bm\theta), y)$ for some loss function $\ell$ and model $g$ with learnable parameters $\bm\theta$. $\bm D_b$ is a subset of dataset $\bm D$ of cardinality $b$.
  \item Define how many passes over the data to make (each one known as an Epoch)
  \item Define a learning rate $\eta$
\end{itemize}

Mini-batch Gradient Descent updates the parameters $\bm\theta$ by moving them in the direction of the negative gradient with respect to the loss of a \textbf{mini-batch} $\bm D_b$, $\mathcal{L}_b$ by the learning rate $\eta$ multiplied by the gradient:
\\[1em]
\hspace{1cm}partition the dataset $\bm D$ into an array of subsets of size $b$\\
\hspace{1cm} \texttt{for each Epoch:}\\
  \hspace{2cm} \texttt{for each $\bm D_b \in partitioned(\bm D$):}\\
      \hspace{3cm} $\bm\theta \leftarrow \bm\theta - \eta \nabla_{\bm\theta} \mathcal{L}_b$
\end{frame}

%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{Mini-batch Stochastic Gradient Descent}
\begin{itemize}
	\item<1-> Mini-batch gradient descent seeks to find a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.
  \item<1-> The model update frequency is higher than  gradient descent, which allows for a more robust convergence, avoiding local minima.
  \item<1-> Allows for computationally efficiency (good utilisation of resources)
  \item<2-> The batching allows both the efficiency of not having all training data in memory and algorithm implementations.
\end{itemize}
\end{frame}

%-------------------------------------------------------------%

\begin{frame}\frametitle{So, what about the learning rate?}

\begin{itemize}
\item<+-> Choice of learning rate is extremely important
\item<+-> But we have to reason about the `loss landscape'
\begin{itemize}
  \item<+-> Most convergence analysis of optimisation algorithms assumes a convex loss landscape
  \begin{itemize}
    \item Easy to reason about
    \item Can be shown that (S)GD will converge to the optimal solution for a variety of learning rates
    \item Can give insights into potential problems in the non-convex case
  \end{itemize}
  \item<+-> Deep Learning is highly non-convex
  \begin{itemize}
    \item<+-> Many local minima
    \item<+-> Plateaus
    \item<+-> Saddle points
    \item<+-> Symmetries (permutation, etc)
    \item<+-> Certainly no single global minima
  \end{itemize}
\end{itemize}
\end{itemize}
\end{frame}

%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{*GD in the convex case: failure modes}

%\begin{itemize}
% \item If the ellipse is very elongated, the direction of steepest descent is almost perpendicular to the direction towards the minimum
% \item The gradient vector will have a large component along the short axis of the ellipse and a small component along the long axis of the ellipse.
% \item This is the opposite of what we want to optimise efficiently
%\end{itemize}

\includegraphics[width=\textwidth]{contours.pdf}

\end{frame}

%-------------------------------------------------------------%

\begin{frame}\frametitle{Accelerated Gradient Methods}

\begin{itemize}
  \item<+-> Accelerated gradient methods use an exponentially weighted average of the gradient, rather than the instantaneous gradient estimate at each step.
  \item<+-> A physical analogy would be one of the momentum a ball picks up rolling down a hill...
  \item<+-> As you'll see, this helps address the *GD failure modes, but also helps avoid getting stuck in local minima
\end{itemize}

\end{frame}

%-------------------------------------------------------------%
\begin{frame}{pause}\frametitle{Exponentially Weighted Averages}
	
	$v_t = \beta v_{t-1} + (1 - \beta) \theta_t$ \vspace{3mm} \\ \pause
	
	$v_t$ is approximately average over $\approx \frac{1}{1-\beta}$ days \vspace{6mm} \\ \pause
	
	For example \\
	$v_{100} = 0.9 v_{99} + 0.1 \theta_{100}$ \\
	$v_{99} = 0.9 v_{98} + 0.1 \theta_{99}$ \\
	$v_{98} = 0.9 v_{97} + 0.1 \theta_{98}$ \\
	\dots \vspace{3mm} \\ \pause
	$v_{100} = 0.1 \theta_{100} + 0.9 [0.1 \theta_{99} + 0.9 [\dots]]]$   \vspace{2mm} \\ \pause
	$v_{100} = 0.1 \theta_{100} + 0.9 * 0.1 * \theta_{99} + 0.1 * (0.9)^2 * \theta_{98} + 0.1 (0.9)^3 \theta_{97} + \dots $ 
\end{frame}

%-------------------------------------------------------------%

\begin{frame}{pause}\frametitle{Momentum I}

It's common for the weighted (or `leaky') term (the `velocity', $v_t$) to be a weighted average of the instantaneous gradient $g_t$ and the past velocity\footnote{There are quite a few variants of this; here we're following the PyTorch variant}:

\begin{equation*}
  v_t = \beta v_{t-1} + g_t
\end{equation*}

where $\beta \in [0,1]$ is the `momentum'.

\end{frame}
%-------------------------------------------------------------%


%\begin{frame}[fragile]\frametitle{ Motivating Momentum}
%\url{https://www.youtube.com/watch?v=7HZk7kGk5bU&t=172s}
%\end{frame}
%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{Momentum II}
\begin{itemize}
\item The momentum method allows to accumulate velocity in directions of low curvature that
persist across multiple iterations
\item This leads to accelerated progress in low curvature directions compared to gradient descent %\footnote{\url{http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf}}
\end{itemize}
\end{frame}
%-------------------------------------------------------------%

\begin{frame}[fragile]\frametitle{MB-SGD with Momentum}

Learning with momentum on iteration $t$ (batch at $t$ denoted by $b(t)$) is given by:
\begin{align*}
\bm v_t & \leftarrow \beta \bm v_{t-1} + \nabla_{\bm\theta} \mathcal{L}_{b(t)} \\
\bm\theta_t & \leftarrow \bm\theta_{t-1} - \eta \bm v_t
\end{align*}

Note $\beta = 0.9$ is a good choice for the momentum parameter.
\end{frame}
%-------------------------------------------------------------%

\begin{frame}\frametitle{}

\end{frame}
%-------------------------------------------------------------%
\begin{frame}\frametitle{Learning rate schedules}

\begin{itemize}
  \item<+-> In practice you want to decay your learning rate over time
  \item<+-> Smaller steps will help you get closer to the minima
  \item<+-> But don't do it to early, else you might get stuck
  \item<+-> Something of an art form!
  \begin{itemize}
    \item `Grad Student Descent' or GDGS (`Gradient Descent by Grad Student`)
  \end{itemize}
\end{itemize}

\end{frame}
%-------------------------------------------------------------%
\begin{frame}\frametitle{Reduce LR on plateau}
\begin{itemize}
  \item Common Heuristic approach:
  \begin{itemize}
    \item if the loss hasn't improved (within some tolerance) for $k$ epochs
    \item then drop the lr by a factor of 10
  \end{itemize}
  \item Remarkably powerful!
\end{itemize}

%\begin{center}
%\begin{tikzpicture}
%  \begin{axis}[
%    height=6cm,
%    width=12cm,
%    grid=both,
%    xtick distance=1,
%    ytick distance=1,
%    xticklabels={,,},
%    yticklabels={,,},
%    axis equal,
%    xmin=0,
%    xmax=5,
%    ymin=0,
%    ymax=10 
%  ]
%  \end{axis}
%\end{tikzpicture}
%\end{center}

\end{frame}

%-------------------------------------------------------------%

\begin{frame}\frametitle{Cyclic learning rates}
\begin{itemize}
  \item<+-> Worried about getting stuck in a non-optimal local minima?
  \item<+-> Cycle the learning rate up and down (possibly annealed), with a different lr on each batch
  \item<+-> See \url{https://arxiv.org/abs/1506.01186}
\end{itemize}
\end{frame}

%-------------------------------------------------------------%
\begin{frame}\frametitle{More advanced optimisers}

\begin{itemize}
  \item<+-> Adagrad
  \begin{itemize}
    \item Decrease learning rate dynamically per weight.
    \item Squared magnitude of the gradient used to adjust how quickly progress is made - weights with large gradients are compensated with a smaller learning rate.
    \item Used for less complex models, too slow for deep learning.
  \end{itemize}
  \item<+-> RMSProp
  \begin{itemize}
    \item Modifies Adagrad to decouple learning rate from gradient magnitude scaling
    \item Incorporates leaky averaging of squared gradient magnitudes
    \item LR would typically follow a predefined schedule
  \end{itemize}
  \item<+-> Adam
  \begin{itemize}
    \item Essentially takes all the best ideas from RMSProp and SDG+Momentum
    \item Shown that it might still diverge (or be non optimal, even in convex settings)...

  \end{itemize}
\end{itemize}

\end{frame}
%-------------------------------------------------------------%

\begin{frame}\frametitle{Take-away messages}

\begin{itemize}
  \item The loss landscape of a deep network is complex to understand (and is far from convex)
  \item If you're in a hurry to get results use Adam
  \item If you have time (or a Grad Student at hand), then use SGD (with momentum) and work on tuning the learning rate
  \item If you're implementing something from a paper, then follow what they did!
\end{itemize}

\end{frame}
%-------------------------------------------------------------%

% \begin{frame}[fragile]\frametitle{RMSProp}
% Learning with RMSProp is given by \vspace{5mm}

% On iteration $t$: \\
% \hspace{3mm} Compute $dW$ on current mini-batch \\
% \begin{eqnarray}
% && S_{dW_t} =  \beta S_{dW_{t-1}} + (1 - \beta) dW_t^2 \\
% && w_t = w_{t-1} - \eta \frac{dW_t}{\sqrt S_{dW_t}} 
% \end{eqnarray}

% \end{frame}
% %-------------------------------------------------------------%


% \begin{frame}{pause}\frametitle{Bias Correction Motivation}

% \begin{itemize}
% \item Let's  assume that $v_0 = 0$ and $\beta = 0.9$ and we're considering exponentially weighted averages \pause
% \item It follows that $v_1 = \beta (0) + (1 - \beta) \theta_1 = 0.1 \hspace{1mm} \theta_1$  \pause
% \item and $v_2 = \beta ( (1 - \beta) \theta_1) + (1-\beta) \theta_2 = 0.0196 \hspace{1mm} \theta_1 + 0.02 \hspace{1mm} \theta_2 $ \pause
% \end{itemize}
% \end{frame}
% %-------------------------------------------------------------%

% \begin{frame}{pause}\frametitle{Bias Correction }

% \begin{itemize}
% \item Add a bias correction term: $\frac{v_t}{1 - \beta^t} $\pause
% \item $t = 1$: $\frac{v_1}{1 - (0.9)^1} = 10 * v_1$ \pause
% \item $t = 2$: $\frac{v_2}{1 - (0.9)^2} = 5.263 * v_2$ \pause
% \item \dots
% \item $t = 10$: $\frac{v_{10}}{1 - (0.9)^{10}} = 1.535 * v_{10}$ 
% \item \dots
% \item $t = 20$: $\frac{v_{20}}{1 - (0.9)^{20}} = 1.138 * v_{20}$ 
% \end{itemize}
% \end{frame}
% %-------------------------------------------------------------%

% \begin{frame}[fragile]\frametitle{ Adam}
% Initialize parameters: $V_{dW} = 0, S_{dW} = 0$ \vspace{3mm}

% On iteration $t$: \\
% \hspace{3mm} Compute $dW_t$ on current mini-batch \\
% \begin{eqnarray}
% && V_{dW} =  \beta_1 V_{dW} + (1 - \beta_1) dW, \hspace{3mm} V_{dW}^{corr} = \frac{ V_{dW}} { (1 - \beta_1^t)}\\ 
% && S_{dW} =  \beta_2 S_{dW} + (1 - \beta_2) dW^2 , \hspace{3mm} S_{dW}^{corr} = \frac{ S_{dW}} { (1 - \beta_2^t)}\\ 
% && w := w - \eta \frac{V_{dW}^{corr}}{\sqrt (S_{dW}^{corr} + \epsilon)}
% \end{eqnarray}

% \end{frame}
%-------------------------------------------------------------%


\end{document}
