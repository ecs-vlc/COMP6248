{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "76f331e42b18f90c2798827fd09f7916",
     "grade": false,
     "grade_id": "cell-64f625d2f2a95421",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Stochastic Gradient Descent, Momentum, and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7f0ff2c2c93f9726d6df73febe04ee97",
     "grade": false,
     "grade_id": "cell-4298e9d3fced7f34",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### For this part of the lab, using the same data and setup as in Part 1, compare how stochastic gradient descent (SGD), SGD with momentum, and ADAM will perform. You are free to write all of the code on your own, or if you prefer, you can fill in the missing sections in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1c4e30f2dcd1eaf1c6aa91f689bf6d7",
     "grade": false,
     "grade_id": "cell-d5a58ae230138c8a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Start by reusing some of the functions you have coded for Part 1 to compute the hypothesis, the gradient of the cost function, and the cost function. This code will then be used by the next section of the workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "14db73c902e1aba6fd752b6e28234498",
     "grade": true,
     "grade_id": "cell-f09153a723d1bb21",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## generate M data points roughly forming a line (noise added)\n",
    "M = 50\n",
    "theta_true = torch.Tensor([[0.5], [2]])\n",
    "\n",
    "X = 10 * torch.rand(M, 2) - 5\n",
    "X[:, 1] = 1.0\n",
    "\n",
    "y = torch.mm(X, theta_true) + 0.3 * torch.randn(M, 1)\n",
    "\n",
    "## hypothesis computes $h_theta$\n",
    "def hypothesis(theta, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "## grad_cost_func computes the gradient of J for linear regression given J is the MSE \n",
    "def grad_cost_func(theta, X, y): \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "## cost_func computes\n",
    "def cost_func(theta, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "34e932803ed631e6d14a1f09103b98c2",
     "grade": false,
     "grade_id": "cell-4f05371072d188fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Now we'd like to compare how stochastic gradient descent (SGD), SGD with momentum, and ADAM will run over weight updates. Complete the defined functions below and make sure to add the parameters which may need to be passed into the functions as well. Plot the weight updates over the contour plot using varying colours. Note, for clarity, you may only want to plot some of the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc258f32732e143ee8c8ad9b25b34096",
     "grade": true,
     "grade_id": "cell-5189819c97d93617",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "## The weight update computed using the ADAM optimisation algorithm\n",
    "def weightupdate_adam(count, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "## The weight update computed using SGD + momentum\n",
    "def weightupdate_sgd_momentum(count, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "## The weight updated computed using SGD\n",
    "def weigthupdate_sgd(count, X, y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "N = 200\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "alpha = 0.01\n",
    "\n",
    "theta_0 = torch.Tensor([[2],[4]]) #initialise\n",
    "\n",
    "# Write the code that will call of the optimisation update functions and compute weight updates for each individual data point over N iterations.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "theta_0_vals = np.linspace(-2,4,100)\n",
    "theta_1_vals = np.linspace(0,4,100)\n",
    "theta = torch.Tensor(len(theta_0_vals),2)\n",
    "\n",
    "# Compute the value of the cost function, J, over all the thetas in order to plot the contour below.\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "xc,yc = np.meshgrid(theta_0_vals, theta_1_vals)\n",
    "contours = plt.contour(xc, yc, J, 20)\n",
    "\n",
    "# Now plot the output of SGD, momentum and Adam all on the same plot for comparison\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
